---
phase: 03-llm-text-generation-pipeline
plan: 02
type: execute
wave: 2
depends_on:
  - "03-01"
files_modified:
  - src/lib/llm/prompts.ts
  - src/lib/llm/generate-session.ts
  - src/lib/llm/index.ts
autonomous: true
requirements:
  - VOIC-01
  - VOIC-06
user_setup:
  - service: openai
    why: "LLM text generation via Responses API"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API keys (already configured in Phase 2 for Moderation API)"

must_haves:
  truths:
    - "LLM generates wellness session text in real-time via OpenAI Responses API streaming"
    - "Token deltas are accumulated into sentences via the sentence chunker from Plan 01"
    - "Every complete sentence passes through checkContentSafety before being yielded downstream"
    - "Unsafe sentences are replaced with wellness fallbacks without breaking the generator"
    - "Stream errors are caught and yield a graceful fallback instead of crashing"
  artifacts:
    - path: "src/lib/llm/prompts.ts"
      provides: "Session prompt templates combining SAFETY_SYSTEM_PROMPT with session-specific instructions"
      exports: ["buildSessionInstructions", "SESSION_PROMPT"]
      min_lines: 15
    - path: "src/lib/llm/generate-session.ts"
      provides: "Three-stage async generator pipeline: LLM stream -> sentence chunker -> safety filter"
      exports: ["generateSession", "streamLlmTokens", "chunkBySentence", "filterSafety"]
      min_lines: 60
    - path: "src/lib/llm/index.ts"
      provides: "Barrel exports for the LLM module"
      exports: ["generateSession"]
      min_lines: 3
  key_links:
    - from: "src/lib/llm/prompts.ts"
      to: "src/lib/safety/index.ts"
      via: "import { SAFETY_SYSTEM_PROMPT }"
      pattern: "SAFETY_SYSTEM_PROMPT"
    - from: "src/lib/llm/generate-session.ts"
      to: "src/lib/llm/prompts.ts"
      via: "import { buildSessionInstructions }"
      pattern: "buildSessionInstructions"
    - from: "src/lib/llm/generate-session.ts"
      to: "src/lib/llm/sentence-chunker.ts"
      via: "import { splitAtSentenceBoundaries }"
      pattern: "splitAtSentenceBoundaries"
    - from: "src/lib/llm/generate-session.ts"
      to: "src/lib/safety/index.ts"
      via: "import { checkContentSafety }"
      pattern: "checkContentSafety"
    - from: "src/lib/llm/generate-session.ts"
      to: "openai SDK"
      via: "client.responses.create({ stream: true })"
      pattern: "responses\\.create"
---

<objective>
Build the three-stage async generator streaming pipeline: LLM token stream -> sentence chunker -> safety filter.

Purpose: This is the core text generation pipeline. The OpenAI Responses API streams token deltas, the sentence chunker (from Plan 01) accumulates them into complete sentences, and the existing safety filter (from Phase 2) inspects each sentence before yielding it downstream. Phase 4 will consume this pipeline to feed sentences to TTS.

Output: `src/lib/llm/prompts.ts`, `src/lib/llm/generate-session.ts`, `src/lib/llm/index.ts`
</objective>

<execution_context>
@/Users/torbjorntest/.claude/get-shit-done/workflows/execute-plan.md
@/Users/torbjorntest/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-text-generation-pipeline/03-RESEARCH.md
@.planning/phases/03-llm-text-generation-pipeline/03-01-SUMMARY.md
@src/lib/safety/index.ts
@src/lib/safety/system-prompt-safety.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create session prompt templates and streaming pipeline</name>
  <files>src/lib/llm/prompts.ts, src/lib/llm/generate-session.ts, src/lib/llm/index.ts</files>
  <action>
Create three files in `src/lib/llm/`:

**1. `prompts.ts` — Session prompt templates**

Export a `SESSION_PROMPT` constant containing the wellness guide persona instructions:
- Speak in second person, use sensory language, pace words for listening with eyes closed
- Each response should be 3-5 sentences of guided content
- Warm, present, intimate tone

Export a `buildSessionInstructions(sessionContext?: string): string` function that:
- Combines `SAFETY_SYSTEM_PROMPT` (imported from `@/lib/safety`) with `SESSION_PROMPT`
- Appends optional `sessionContext` parameter (for phase-specific instructions from Phase 5 later)
- Joins with `\n\n` separators
- Returns the combined string for the `instructions` parameter of the Responses API

**2. `generate-session.ts` — Three-stage async generator pipeline**

Create a module-level OpenAI client singleton: `const openai = new OpenAI()` (reads OPENAI_API_KEY from process.env automatically, matching the pattern used in `src/lib/safety/moderation.ts`).

Define a configurable model constant: `const DEFAULT_MODEL = "gpt-4.1-mini"` and default temperature: `const DEFAULT_TEMPERATURE = 0.8`. Also define a safety cap: `const MAX_OUTPUT_TOKENS = 4096`.

Export four async generator functions:

`streamLlmTokens(sessionPrompt: string, options?: { model?: string, temperature?: number }): AsyncGenerator<string>`
- Calls `openai.responses.create()` with `stream: true`
- Uses `instructions` from `buildSessionInstructions(sessionPrompt)`
- Uses `input: [{ role: "user", content: "Begin the session." }]`
- Sets `model`, `temperature`, and `max_output_tokens` from options/defaults
- Iterates with `for await (const event of stream)` and yields `event.delta` when `event.type === "response.output_text.delta"`
- Wraps the entire iteration in try/catch: on error, import `getRandomFallback` from `@/lib/safety` and yield a fallback message, then return (do NOT re-throw)

`chunkBySentence(tokens: AsyncGenerator<string>, minLength?: number): AsyncGenerator<string>`
- Accumulates token deltas into a buffer string
- On each token, appends to buffer, then calls `splitAtSentenceBoundaries(buffer, minLength)` from `./sentence-chunker`
- Yields each complete sentence from the result
- Sets buffer to the remainder
- After the loop, flushes any remaining buffer content with `buffer.trim()` if non-empty

`filterSafety(sentences: AsyncGenerator<string>): AsyncGenerator<string>`
- For each sentence, calls `checkContentSafety(sentence)` from `@/lib/safety`
- Yields `result.output` (guaranteed non-empty by the safety module's contract)
- No try/catch needed here — checkContentSafety handles its own errors internally

`generateSession(sessionPrompt: string, options?: { model?: string, temperature?: number, minSentenceLength?: number }): AsyncGenerator<string>`
- Composes the three stages: `streamLlmTokens` -> `chunkBySentence` -> `filterSafety`
- Uses `yield*` to delegate to the final stage
- This is the primary public API that Phase 4 will consume

**3. `index.ts` — Barrel exports**

Re-export from generate-session.ts: `generateSession`, `streamLlmTokens`, `chunkBySentence`, `filterSafety`
Re-export from prompts.ts: `buildSessionInstructions`, `SESSION_PROMPT`
Re-export from sentence-chunker.ts: `splitAtSentenceBoundaries`, `SplitResult`

Use TypeScript path aliases (`@/lib/...`) for all cross-module imports. Use relative imports (`./...`) for intra-module imports within `src/lib/llm/`.
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no TypeScript errors
    - All imports resolve correctly (safety module, sentence-chunker, openai SDK)
    - `node -e "const m = require('./src/lib/llm/index'); console.log(Object.keys(m))"` is NOT expected to work (ESM modules), but TypeScript compilation confirms correctness
  </verify>
  <done>
    - prompts.ts exports buildSessionInstructions that combines SAFETY_SYSTEM_PROMPT + SESSION_PROMPT + optional context
    - generate-session.ts exports four async generators composing the LLM -> chunker -> safety pipeline
    - OpenAI client is a module-level singleton using Responses API with stream: true
    - Stream errors yield a fallback message instead of crashing
    - index.ts barrel re-exports all public API from the llm module
    - No TypeScript compilation errors
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes — all types resolve
- Sentence chunker tests still pass: `npx vitest run src/lib/llm/__tests__/sentence-chunker.test.ts`
- Pipeline structure is correct: generateSession composes streamLlmTokens -> chunkBySentence -> filterSafety
- Safety integration: filterSafety imports and calls checkContentSafety from @/lib/safety
- Prompt integration: buildSessionInstructions imports SAFETY_SYSTEM_PROMPT from @/lib/safety
- Error handling: streamLlmTokens has try/catch that yields fallback on error
</verification>

<success_criteria>
- LLM streaming uses OpenAI Responses API (`client.responses.create({ stream: true })`) with `instructions` parameter
- Token deltas flow through sentence chunker into safety filter as a composed async generator pipeline
- Every sentence is safety-filtered via the existing checkContentSafety before being yielded
- Unsafe content is transparently replaced with fallbacks (consumer never sees unsafe text)
- Stream errors produce graceful fallback output instead of unhandled exceptions
- All code compiles without TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-text-generation-pipeline/03-02-SUMMARY.md`
</output>
