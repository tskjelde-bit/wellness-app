---
phase: 02-safety-consent-framework
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/safety/index.ts
  - src/lib/safety/moderation.ts
  - src/lib/safety/keyword-blocklist.ts
  - src/lib/safety/crisis-detector.ts
  - src/lib/safety/constants.ts
  - src/lib/safety/system-prompt-safety.ts
autonomous: true
requirements: [SAFE-04, SAFE-05, SAFE-06]

must_haves:
  truths:
    - "checkContentSafety function accepts a text string and returns SafetyCheckResult with safe/unsafe status, original text, output text (original or fallback), and blockedBy reason"
    - "OpenAI Moderation API wrapper calls omni-moderation-latest model and returns structured ModerationResult with flagged, categories, scores, and isCrisis fields"
    - "Keyword blocklist uses word-boundary regex matching against a domain-specific term list and returns blocked/not-blocked with matched term"
    - "Crisis detector identifies self-harm language via keyword list and returns helpline resource message"
    - "Unsafe content is ALWAYS replaced with a wellness-appropriate fallback -- never returns empty or error"
    - "System prompt safety template provides LLM instructions for staying within wellness boundaries"
  artifacts:
    - path: "src/lib/safety/index.ts"
      provides: "Composed three-layer content safety filter"
      exports: ["checkContentSafety", "SafetyCheckResult"]
    - path: "src/lib/safety/moderation.ts"
      provides: "OpenAI Moderation API wrapper"
      exports: ["moderateContent", "ModerationResult"]
    - path: "src/lib/safety/keyword-blocklist.ts"
      provides: "Domain-specific keyword filter"
      exports: ["checkKeywordBlocklist", "KeywordCheckResult"]
    - path: "src/lib/safety/crisis-detector.ts"
      provides: "Crisis language detection with helpline response"
      exports: ["detectCrisisKeywords", "CrisisDetectionResult"]
    - path: "src/lib/safety/constants.ts"
      provides: "Fallback responses and keyword blocklist terms"
      exports: ["SAFETY_FALLBACKS", "getRandomFallback", "KEYWORD_BLOCKLIST"]
    - path: "src/lib/safety/system-prompt-safety.ts"
      provides: "LLM system prompt safety instructions for Phase 3"
      exports: ["SAFETY_SYSTEM_PROMPT"]
  key_links:
    - from: "src/lib/safety/index.ts"
      to: "src/lib/safety/moderation.ts"
      via: "moderateContent call for Layer 2 classification"
      pattern: "moderateContent\\(text\\)"
    - from: "src/lib/safety/index.ts"
      to: "src/lib/safety/keyword-blocklist.ts"
      via: "checkKeywordBlocklist call for Layer 3 filtering"
      pattern: "checkKeywordBlocklist\\(text\\)"
    - from: "src/lib/safety/index.ts"
      to: "src/lib/safety/constants.ts"
      via: "getRandomFallback for unsafe content replacement"
      pattern: "getRandomFallback\\(\\)"
    - from: "src/lib/safety/moderation.ts"
      to: "openai SDK"
      via: "openai.moderations.create() API call"
      pattern: "openai\\.moderations\\.create"
---

<objective>
Build the three-layer content safety pipeline as a standalone module.

Purpose: Create the safety filtering infrastructure that Phase 3 (LLM text generation) will wire into the streaming pipeline. Layer 1 is the system prompt template (used at LLM call time). Layer 2 is the OpenAI Moderation API wrapper (classifies each sentence). Layer 3 is a custom keyword blocklist (catches domain-specific terms). The composed `checkContentSafety` function runs Layers 2 and 3, with Layer 1 provided as a template for upstream use.

Output: Complete `src/lib/safety/` module with moderation API wrapper, keyword blocklist, crisis detector, fallback responses, system prompt template, and composed safety filter.
</objective>

<execution_context>
@/Users/torbjorntest/.claude/get-shit-done/workflows/execute-plan.md
@/Users/torbjorntest/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-safety-consent-framework/02-RESEARCH.md

@src/lib/env.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI Moderation API wrapper, keyword blocklist, and crisis detector</name>
  <files>
    src/lib/safety/moderation.ts
    src/lib/safety/keyword-blocklist.ts
    src/lib/safety/crisis-detector.ts
    src/lib/safety/constants.ts
  </files>
  <action>
**Install openai package:**

Run `npm install openai` to add the OpenAI SDK. This is needed for the Moderation API and will also be used for LLM generation in Phase 3.

**Create src/lib/safety/constants.ts:**

1. Export `SAFETY_FALLBACKS` array with 6-8 wellness-appropriate fallback responses that can replace blocked content without breaking session immersion. Each should be 1-2 sentences of calming, grounding language (breathing focus, body warmth, grounding, peaceful imagery). Follow Research Pattern 5 -- these replace unsafe content, they are NOT error messages.

2. Export `getRandomFallback()` function that returns a random fallback from SAFETY_FALLBACKS.

3. Export `KEYWORD_BLOCKLIST` as a string array with 30-50 domain-specific terms. These are terms that the moderation API might miss because they are domain-adjacent to wellness content but cross the product's content boundary. Categories to include:
   - Explicit sexual acts and graphic sexual terms (the product allows "intimate wellness" but NOT explicit sexual content)
   - Graphic violence terms
   - Substance abuse instruction terms
   - Terms that could indicate the user is trying to redirect the AI toward therapy/clinical claims
   Use lowercase. These will be matched with word-boundary regex.

4. Export `CRISIS_KEYWORDS` array with 10-15 crisis/self-harm keyword phrases: "kill myself", "want to die", "end my life", "suicide", "self harm", "self-harm", "cutting myself", "hurt myself", "no reason to live", "better off dead", "end it all", "not worth living".

**Create src/lib/safety/moderation.ts:**

Follow Research Pattern 4 exactly.

1. Import `OpenAI` from `openai` -- instantiate with no arguments (reads OPENAI_API_KEY from env automatically).

2. Export `ModerationResult` interface:
   ```typescript
   interface ModerationResult {
     flagged: boolean;
     categories: Record<string, boolean>;
     scores: Record<string, number>;
     isCrisis: boolean;
   }
   ```

3. Export `moderateContent(text: string): Promise<ModerationResult>` function:
   - Call `openai.moderations.create({ model: "omni-moderation-latest", input: text })`
   - Extract `result.categories` and `result.category_scores` from `response.results[0]`
   - Set `isCrisis = true` if any of these categories are true: `self-harm`, `self-harm/intent`, `self-harm/instructions`
   - Apply custom threshold for `sexual` category: only flag if score >= 0.8 (to avoid false positives on legitimate wellness content like body awareness language). For all other categories, use the API's default `flagged` boolean.
   - Construct a custom `flagged` boolean: true if any non-sexual category is flagged by the API, OR if sexual score >= 0.8

**Create src/lib/safety/keyword-blocklist.ts:**

Follow Research code example exactly.

1. Export `KeywordCheckResult` interface: `{ blocked: boolean; matchedTerm: string | null }`
2. Export `checkKeywordBlocklist(text: string): KeywordCheckResult` function:
   - Convert text to lowercase
   - For each term in KEYWORD_BLOCKLIST, create a word-boundary regex: `new RegExp(\`\\b${escapeRegex(term)}\\b\`, "i")`
   - Return `{ blocked: true, matchedTerm: term }` on first match
   - Return `{ blocked: false, matchedTerm: null }` if no match
3. Include the `escapeRegex` helper function (escapes special regex characters)

**Create src/lib/safety/crisis-detector.ts:**

Follow Research Pattern 7 exactly.

1. Import `HELPLINE_RESOURCES` from `@/lib/consent/constants` (created by Plan 01) and `CRISIS_KEYWORDS` from `./constants`
2. Export `CrisisDetectionResult` interface: `{ detected: boolean; helplineResponse: string | null }`
3. Export `detectCrisisKeywords(text: string): CrisisDetectionResult`:
   - Convert to lowercase, check if any CRISIS_KEYWORDS phrase is found via `.includes()`
   - If detected, build helpline message referencing 988 Lifeline and SAMHSA (see Research Pattern 7 `buildHelplineMessage`)
   - If not detected, return `{ detected: false, helplineResponse: null }`

IMPORTANT: crisis-detector.ts imports from `@/lib/consent/constants` which is created by Plan 01. If Plan 02 executes in parallel with Plan 01, the import path must be correct. Since both plans are Wave 1, the executor should handle this. If the file does not exist yet, create a minimal version of HELPLINE_RESOURCES in safety/constants.ts as a fallback and add a TODO comment to import from consent/constants once available. Prefer the consent/constants import if the file exists.
  </action>
  <verify>
1. `npm ls openai` confirms openai package is installed
2. All four files exist and have correct exports
3. `npx tsc --noEmit` passes (type checking)
4. KEYWORD_BLOCKLIST has 30+ terms
5. CRISIS_KEYWORDS has 10+ phrases
6. SAFETY_FALLBACKS has 6+ responses
7. moderateContent function signature matches ModerationResult interface
  </verify>
  <done>
Three safety modules created: OpenAI Moderation API wrapper with custom sexual-category threshold (0.8), domain-specific keyword blocklist with word-boundary matching, and crisis language detector with helpline response generation. All fallback responses are wellness-appropriate and immersion-preserving.
  </done>
</task>

<task type="auto">
  <name>Task 2: Compose three-layer safety filter and create system prompt safety template</name>
  <files>
    src/lib/safety/index.ts
    src/lib/safety/system-prompt-safety.ts
  </files>
  <action>
**Create src/lib/safety/system-prompt-safety.ts:**

Follow Research code example exactly. Export `SAFETY_SYSTEM_PROMPT` as a string constant containing the LLM system prompt safety instructions. This is Layer 1 of the three-layer filter -- applied upstream at LLM call time (Phase 3), not by the safety filter function.

Content must include:
1. Allowed domains: breathing exercises, body awareness, guided relaxation, sensory awareness, emotional check-ins
2. Prohibited content: sexually explicit, violent, substance abuse, medical advice, therapeutic claims
3. Redirect instructions: do NOT acknowledge inappropriate requests, gently redirect with "Let's bring our focus back to..." phrasing, maintain warm calm tone
4. Crisis response: respond with compassion, provide 988 Lifeline info, do NOT attempt therapy
5. AI identity: clearly state "I am an AI guide" if asked

Use the exact text from Research Pattern (system-prompt-safety.ts code example) as the starting point.

**Create src/lib/safety/index.ts:**

This is the composed safety filter -- the main entry point for content safety checking.

1. Export `SafetyCheckResult` interface:
   ```typescript
   interface SafetyCheckResult {
     safe: boolean;
     original: string;
     output: string;        // ALWAYS a string -- original if safe, fallback if blocked
     moderationResult: ModerationResult | null;
     crisisDetected: boolean;
     blockedBy: "none" | "moderation" | "keyword" | "crisis";
   }
   ```

2. Export `checkContentSafety(text: string): Promise<SafetyCheckResult>`:
   - Layer 1 (system prompt) is applied upstream -- this function handles Layers 2 and 3
   - **Step 1: Crisis detection first** (highest priority). Run `detectCrisisKeywords(text)`. If detected, return immediately with `blockedBy: "crisis"`, `crisisDetected: true`, and the helpline response as `output` (NOT a random fallback -- the helpline message is the output).
   - **Step 2: OpenAI Moderation API** (Layer 2). Run `moderateContent(text)`. If `isCrisis` is true on the moderation result, treat as crisis (same as Step 1 but with moderation-detected crisis). If `flagged` is true (non-crisis), return with `blockedBy: "moderation"`, `output: getRandomFallback()`.
   - **Step 3: Keyword blocklist** (Layer 3). Run `checkKeywordBlocklist(text)`. If blocked, return with `blockedBy: "keyword"`, `output: getRandomFallback()`.
   - **Step 4: Content is safe.** Return `safe: true`, `output: text`, `blockedBy: "none"`.

   CRITICAL: The `output` field ALWAYS contains a non-empty string. Never return empty string, null, or undefined. This ensures the downstream pipeline (TTS in Phase 4) always has content to speak.

3. Re-export key types and the SAFETY_SYSTEM_PROMPT:
   ```typescript
   export { moderateContent, type ModerationResult } from "./moderation";
   export { checkKeywordBlocklist, type KeywordCheckResult } from "./keyword-blocklist";
   export { detectCrisisKeywords, type CrisisDetectionResult } from "./crisis-detector";
   export { SAFETY_SYSTEM_PROMPT } from "./system-prompt-safety";
   export { SAFETY_FALLBACKS, getRandomFallback } from "./constants";
   ```
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. src/lib/safety/index.ts exports checkContentSafety and SafetyCheckResult
3. src/lib/safety/system-prompt-safety.ts exports SAFETY_SYSTEM_PROMPT
4. SAFETY_SYSTEM_PROMPT contains "988" (crisis helpline reference)
5. SAFETY_SYSTEM_PROMPT contains "AI guide" (identity instruction)
6. checkContentSafety function handles all three code paths: crisis, moderation, keyword
7. `npm run build` succeeds
  </verify>
  <done>
Three-layer content safety filter composed and ready for Phase 3 integration. Layer 1 (system prompt) is a template for LLM configuration. Layers 2 (moderation API) and 3 (keyword blocklist) are executed by checkContentSafety(). Crisis detection runs first as highest priority. All unsafe content is replaced with immersion-preserving fallbacks. The safety module is a standalone library with no UI dependencies.
  </done>
</task>

</tasks>

<verification>
1. `npm install openai` succeeds
2. Complete safety module at src/lib/safety/ with 6 files
3. checkContentSafety composes all three layers correctly
4. Crisis detection has highest priority (checked first)
5. All unsafe content produces a non-empty fallback output
6. System prompt template covers all required safety domains
7. Keyword blocklist has 30+ domain-specific terms with word-boundary matching
8. `npm run build` succeeds
</verification>

<success_criteria>
- openai package installed
- Three-layer safety filter composable and testable as standalone module
- OpenAI Moderation API wrapper uses omni-moderation-latest with custom sexual threshold (0.8)
- Keyword blocklist catches domain-specific terms with word-boundary regex
- Crisis detector identifies self-harm language and generates helpline response
- System prompt template ready for Phase 3 LLM integration
- All blocked content replaced with wellness-appropriate fallbacks (never empty, never error messages)
- TypeScript compiles, production build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/02-safety-consent-framework/02-02-SUMMARY.md`
</output>
