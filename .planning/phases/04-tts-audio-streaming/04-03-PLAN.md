---
phase: 04-tts-audio-streaming
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - src/hooks/use-audio-queue.ts
  - src/hooks/use-session-ws.ts
autonomous: true
requirements:
  - VOIC-05
  - VOIC-04

must_haves:
  truths:
    - "Client plays audio with gap-free continuity using Web Audio API scheduled playback"
    - "AudioContext is created on user gesture (not on page load) to comply with browser autoplay policy"
    - "WebSocket hook connects to /api/session/ws and handles binary audio + JSON control messages"
    - "Client can pause, resume, and end audio playback through the WebSocket hook"
    - "End-to-end pipeline works: generateSession -> TTS -> WebSocket -> client AudioPlaybackQueue"
  artifacts:
    - path: "src/hooks/use-audio-queue.ts"
      provides: "AudioPlaybackQueue class and useAudioQueue hook"
      exports: ["useAudioQueue", "AudioPlaybackQueue"]
    - path: "src/hooks/use-session-ws.ts"
      provides: "useSessionWebSocket hook for WebSocket + audio integration"
      exports: ["useSessionWebSocket"]
  key_links:
    - from: "src/hooks/use-session-ws.ts"
      to: "/api/session/ws"
      via: "WebSocket connection URL"
      pattern: "api/session/ws"
    - from: "src/hooks/use-session-ws.ts"
      to: "src/hooks/use-audio-queue.ts"
      via: "import useAudioQueue"
      pattern: "import.*useAudioQueue.*from.*use-audio-queue"
---

<objective>
Build the client-side audio playback system and WebSocket integration hooks that deliver gap-free streaming audio to the user's browser.

Purpose: This is the last mile of the audio pipeline. The AudioPlaybackQueue uses Web Audio API to schedule decoded MP3 chunks for gap-free playback, while the WebSocket hook manages the connection lifecycle and routes binary audio frames to the queue and JSON messages to React state. Together, they complete the end-to-end pipeline from LLM generation to the user's ears.

Output: AudioPlaybackQueue class with Web Audio API scheduling, useSessionWebSocket hook for React integration.
</objective>

<execution_context>
@/Users/torbjorntest/.claude/get-shit-done/workflows/execute-plan.md
@/Users/torbjorntest/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-tts-audio-streaming/04-RESEARCH.md
@.planning/phases/04-tts-audio-streaming/04-01-PLAN.md
@.planning/phases/04-tts-audio-streaming/04-02-PLAN.md
@src/lib/ws/message-types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AudioPlaybackQueue and useAudioQueue hook</name>
  <files>
    src/hooks/use-audio-queue.ts
  </files>
  <action>
    1. Create `src/hooks/use-audio-queue.ts` with "use client" directive:

    2. Implement `AudioPlaybackQueue` class:
       - Private fields: `audioContext: AudioContext`, `queue: AudioBuffer[]`, `nextPlayTime: number = 0`, `isPlaying: boolean = false`, `isPaused: boolean = false`, `onStateChange: (() => void) | null = null`
       - Constructor: takes `AudioContext` (must be created externally in user gesture handler)
       - `async enqueue(audioData: ArrayBuffer): Promise<void>`:
         - Call `this.audioContext.decodeAudioData(audioData.slice(0))` (slice prevents detached buffer issue -- research pitfall 3)
         - Push decoded AudioBuffer to queue
         - If not playing and not paused, call `this.playNext()`
       - Private `playNext(): void`:
         - If queue empty: set `isPlaying = false`, call `onStateChange?.()`, return
         - Set `isPlaying = true`
         - Shift first buffer from queue
         - Create `AudioBufferSourceNode`, set `.buffer`, connect to `audioContext.destination`
         - Calculate startTime: `Math.max(this.audioContext.currentTime, this.nextPlayTime)` for gap-free scheduling
         - Call `source.start(startTime)`
         - Update `this.nextPlayTime = startTime + buffer.duration`
         - Set `source.onended = () => this.playNext()`
         - Call `onStateChange?.()`
       - `pause(): void`: call `this.audioContext.suspend()`, set `isPaused = true`
       - `resume(): void`: call `this.audioContext.resume()`, set `isPaused = false`, if queue has items and not playing, call `playNext()`
       - `stop(): void`: call `this.audioContext.close()`, clear queue, reset state
       - `get state()`: return `{ isPlaying, isPaused, queueLength: queue.length }`

    3. Implement `useAudioQueue` hook:
       - Use `useRef<AudioPlaybackQueue | null>(null)` for queue instance
       - Use `useState` for `{ isPlaying: boolean, isPaused: boolean, queueLength: number }`
       - Export `initQueue()` function: creates `new AudioContext()` (MUST be called in user gesture handler per research pitfall 1), creates `AudioPlaybackQueue(ctx)`, sets `onStateChange` to update React state
       - Export `enqueue(data: ArrayBuffer)`, `pause()`, `resume()`, `stop()` -- delegates to queue instance
       - Export state values: `isPlaying`, `isPaused`, `queueLength`
       - Use `useEffect` cleanup to call `stop()` on unmount
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no type errors
    - `grep "decodeAudioData" src/hooks/use-audio-queue.ts` confirms Web Audio API usage
    - `grep "AudioBufferSourceNode\|createBufferSource" src/hooks/use-audio-queue.ts` confirms scheduled playback pattern
    - `grep "audioContext.suspend\|audioContext.resume" src/hooks/use-audio-queue.ts` confirms pause/resume
    - `grep "slice(0)" src/hooks/use-audio-queue.ts` confirms ArrayBuffer detachment prevention
  </verify>
  <done>
    AudioPlaybackQueue class decodes MP3 chunks via Web Audio API and schedules them for gap-free playback using AudioBufferSourceNode.start(nextPlayTime). useAudioQueue hook provides React-friendly interface with state tracking. AudioContext created on user gesture to comply with browser autoplay policy.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create useSessionWebSocket hook for end-to-end integration</name>
  <files>
    src/hooks/use-session-ws.ts
  </files>
  <action>
    1. Create `src/hooks/use-session-ws.ts` with "use client" directive:

    2. Import `useAudioQueue` from `./use-audio-queue`
    3. Import `ServerMessage` type from `@/lib/ws/message-types` (shared type used by both client and server)

    4. Implement `useSessionWebSocket` hook:
       - State: `isConnected: boolean`, `currentText: string` (current sentence caption), `sessionId: string | null`, `error: string | null`
       - Refs: `wsRef: useRef<WebSocket | null>(null)`
       - Destructure from `useAudioQueue`: `initQueue`, `enqueue`, `pause: pauseAudio`, `resume: resumeAudio`, `stop: stopAudio`, `isPlaying`, `isPaused`

       - `connect()` callback (called from a user gesture handler like "Start Session" button):
         - Call `initQueue()` to create AudioContext in user gesture context
         - Determine WebSocket URL: `${window.location.protocol === "https:" ? "wss:" : "ws:"}//${window.location.host}/api/session/ws`
         - Create `new WebSocket(url)`
         - Set `ws.binaryType = "arraybuffer"`
         - `ws.onopen`: set `isConnected = true`, clear error
         - `ws.onmessage`: check `event.data`:
           - If `instanceof ArrayBuffer`: call `enqueue(event.data)` to feed audio to playback queue
           - Else (string): parse as `ServerMessage`, switch on type:
             - `"session_start"`: set sessionId
             - `"text"`: set currentText to data
             - `"sentence_end"`: (no-op for now; could clear currentText)
             - `"session_end"`: set isConnected false, call stopAudio
             - `"error"`: set error to message
         - `ws.onclose`: set isConnected false
         - `ws.onerror`: set error to "Connection error"
         - Store in wsRef

       - `startSession(prompt?: string)` callback:
         - Send `JSON.stringify({ type: "start_session", prompt })` via wsRef

       - `pause()` callback: call `pauseAudio()`, send `{ type: "pause" }` via wsRef

       - `resume()` callback: call `resumeAudio()`, send `{ type: "resume" }` via wsRef

       - `endSession()` callback: send `{ type: "end" }` via wsRef, call `stopAudio()`

       - `useEffect` cleanup: close WebSocket and stop audio on unmount

       - Return: `{ connect, startSession, pause, resume, endSession, isConnected, isPlaying, isPaused, currentText, sessionId, error }`
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no type errors
    - `grep "api/session/ws" src/hooks/use-session-ws.ts` confirms WebSocket URL target
    - `grep "binaryType.*arraybuffer" src/hooks/use-session-ws.ts` confirms binary handling
    - `grep "instanceof ArrayBuffer" src/hooks/use-session-ws.ts` confirms binary vs text frame discrimination
    - `grep "useAudioQueue\|initQueue\|enqueue" src/hooks/use-session-ws.ts` confirms audio queue integration
    - `grep "ServerMessage" src/hooks/use-session-ws.ts` confirms shared type import
  </verify>
  <done>
    useSessionWebSocket hook manages WebSocket connection lifecycle, routes binary audio frames to AudioPlaybackQueue for gap-free playback, handles JSON control messages for session state. Provides React-friendly API for connect/start/pause/resume/end with state tracking. End-to-end pipeline: generateSession -> TTS -> WebSocket -> AudioPlaybackQueue -> user hears audio.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes -- all client-side types resolve
- AudioPlaybackQueue uses Web Audio API decodeAudioData + AudioBufferSourceNode scheduling
- useSessionWebSocket connects to /api/session/ws and handles binary + JSON frames
- AudioContext is created inside a user gesture handler (connect function)
- Pause/resume works at both audio and WebSocket levels
- End-to-end chain: LLM -> sentence chunker -> safety filter -> TTS -> WebSocket -> AudioPlaybackQueue -> speakers
</verification>

<success_criteria>
1. AudioPlaybackQueue decodes MP3 chunks and schedules gap-free playback via Web Audio API
2. AudioContext created in user gesture handler (browser autoplay policy compliant)
3. WebSocket hook connects to /api/session/ws and handles binary audio + JSON messages
4. Client can start, pause, resume, and end sessions through the hook
5. Audio state (isPlaying, isPaused) exposed to React components
6. Current sentence text (caption) available for display
7. TypeScript compiles with no errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-tts-audio-streaming/04-03-SUMMARY.md`
</output>
