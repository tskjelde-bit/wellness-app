---
phase: 09-differentiators-polish
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/session/mood-prompts.ts
  - src/lib/session/phase-prompts.ts
  - src/lib/session/orchestrator.ts
  - src/lib/session/index.ts
  - src/lib/tts/elevenlabs-client.ts
  - src/lib/ws/message-types.ts
  - src/lib/ws/session-handler.ts
autonomous: true
requirements: [DIFF-01, DIFF-04]

must_haves:
  truths:
    - "Mood string from client is injected into every LLM phase instruction call"
    - "VoiceId from client overrides the default TTS voice for the entire session"
    - "WebSocket start_session message accepts mood and voiceId fields"
    - "Three curated voice options are defined with IDs, names, and descriptions"
  artifacts:
    - path: "src/lib/session/mood-prompts.ts"
      provides: "Mood-to-prompt mapping for 5 mood states"
      contains: "MOOD_PROMPTS"
    - path: "src/lib/tts/elevenlabs-client.ts"
      provides: "VOICE_OPTIONS array with 3 curated voices"
      contains: "VOICE_OPTIONS"
    - path: "src/lib/session/phase-prompts.ts"
      provides: "buildPhaseInstructions accepting moodContext parameter"
      contains: "moodContext"
    - path: "src/lib/ws/message-types.ts"
      provides: "Extended start_session with mood and voiceId"
      contains: "mood"
  key_links:
    - from: "src/lib/ws/session-handler.ts"
      to: "src/lib/session/orchestrator.ts"
      via: "mood passed in OrchestratorOptions"
      pattern: "mood.*OrchestratorOptions"
    - from: "src/lib/session/orchestrator.ts"
      to: "src/lib/session/phase-prompts.ts"
      via: "moodContext passed to buildPhaseInstructions"
      pattern: "buildPhaseInstructions.*moodContext"
    - from: "src/lib/ws/session-handler.ts"
      to: "src/lib/tts/tts-service.ts"
      via: "voiceId passed to synthesizeSentence options"
      pattern: "voiceId.*synthesizeSentence"
---

<objective>
Add server-side mood adaptation and voice selection to the session pipeline.

Purpose: Enable the AI to adapt session tone based on user's emotional state (DIFF-01) and allow voice selection to pass through the full pipeline (DIFF-04). These server-side changes prepare the protocol and orchestration layer for client integration in Plan 03.

Output: Mood prompt mapping, voice options config, extended WebSocket protocol, orchestrator mood injection, and session handler voiceId passthrough.
</objective>

<execution_context>
@/Users/torbjorntest/.claude/get-shit-done/workflows/execute-plan.md
@/Users/torbjorntest/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-differentiators-polish/09-RESEARCH.md
@.planning/phases/05-session-state-machine-orchestration/05-02-SUMMARY.md
@.planning/phases/05-session-state-machine-orchestration/05-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create mood prompts and voice options config</name>
  <files>
    src/lib/session/mood-prompts.ts
    src/lib/tts/elevenlabs-client.ts
    src/lib/session/phase-prompts.ts
    src/lib/session/index.ts
  </files>
  <action>
1. Create `src/lib/session/mood-prompts.ts`:
   - Export `MOOD_OPTIONS` as a const array of `{ id: string; label: string; emoji: string }` for the 5 moods: "anxious", "sad", "stressed", "neutral", "restless".
   - Export `MOOD_PROMPTS: Record<string, string>` mapping each mood to a 3-5 line prompt modifier following the pattern from research (MOOD CONTEXT / EMPHASIS / TONE SHIFT structure). The "neutral" entry should say "Standard session flow -- balanced across all phases. TONE SHIFT: None."
   - Keep prompts SHORT (3-5 lines) to avoid overwhelming phase-specific instructions (Research Pitfall 5).

2. Add voice options to `src/lib/tts/elevenlabs-client.ts`:
   - Export `VoiceOption` interface: `{ id: string; name: string; description: string; preview: string }`.
   - Export `VOICE_OPTIONS: VoiceOption[]` with 3 voices:
     - Emily: id `"LcfcDJNUP1GQjkzn1xUU"`, preview "Soft & meditative"
     - Rachel: id `"21m00Tcm4TlvDq8ikWAM"`, preview "Warm & steady"
     - George: id `"JBFqnCBsd6RMkjVDRZzb"`, preview "Deep & grounding"
   - Export `DEFAULT_VOICE_ID = VOICE_OPTIONS[0].id` (Emily as new default for wellness).
   - Update `TTS_CONFIG.voiceId` to use `DEFAULT_VOICE_ID`.

3. Extend `buildPhaseInstructions` in `src/lib/session/phase-prompts.ts`:
   - Add optional `moodContext?: string` as the third parameter.
   - If provided, insert the moodContext string BEFORE the `CURRENT PHASE:` line (so phase instructions remain most-recent/salient context per Research Pitfall 5).
   - The existing two-param call signature must remain backward-compatible.

4. Update `src/lib/session/index.ts` barrel to export `MOOD_OPTIONS` and `MOOD_PROMPTS` from mood-prompts.
  </action>
  <verify>
    Run `npx tsc --noEmit` to verify no type errors. Grep for MOOD_PROMPTS and VOICE_OPTIONS exports.
  </verify>
  <done>
    mood-prompts.ts exports 5 mood mappings. elevenlabs-client.ts exports 3 VOICE_OPTIONS with DEFAULT_VOICE_ID. buildPhaseInstructions accepts optional moodContext. All backward-compatible.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend WebSocket protocol, orchestrator, and session handler for mood + voiceId</name>
  <files>
    src/lib/ws/message-types.ts
    src/lib/session/orchestrator.ts
    src/lib/ws/session-handler.ts
  </files>
  <action>
1. Extend `ClientMessage` in `src/lib/ws/message-types.ts`:
   - Add `mood?: string` and `voiceId?: string` to the `start_session` variant.
   - Update `parseClientMessage` to extract `mood` (typeof string) and `voiceId` (typeof string) from the parsed object, passing them through as optional fields.

2. Extend `OrchestratorOptions` in `src/lib/session/orchestrator.ts`:
   - Add `mood?: string` to OrchestratorOptions.
   - In the constructor, store `this.mood = options.mood ?? "neutral"`.
   - Import `MOOD_PROMPTS` from `./mood-prompts`.
   - In the `run()` method, resolve the mood context string: `const moodContext = MOOD_PROMPTS[this.mood] ?? MOOD_PROMPTS["neutral"]`.
   - Pass `moodContext` as the third argument to BOTH `buildPhaseInstructions` calls (the main content call and the wind-down call). This ensures mood adaptation applies to every LLM call throughout the session.

3. Update `session-handler.ts`:
   - In the `start_session` case, extract `message.mood` and `message.voiceId`.
   - Pass `mood` to the `SessionOrchestrator` constructor options.
   - Store `voiceId` as a local variable (e.g., `const selectedVoiceId = message.voiceId || undefined`).
   - Pass `voiceId: selectedVoiceId` in the `synthesizeSentence` options object alongside `previousText` and `signal`. The TTS service already accepts `voiceId` via `SynthesizeOptions` -- no TTS changes needed.
  </action>
  <verify>
    Run `npx tsc --noEmit` to verify no type errors. Verify that the mood param flows: message-types -> session-handler -> orchestrator -> buildPhaseInstructions. Verify voiceId flows: message-types -> session-handler -> synthesizeSentence.
  </verify>
  <done>
    WebSocket accepts mood and voiceId in start_session. Orchestrator injects mood context into every phase LLM call. Session handler passes voiceId to every synthesizeSentence call. Full server-side pipeline for DIFF-01 and DIFF-04 complete.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with no errors
- MOOD_PROMPTS has entries for all 5 moods: anxious, sad, stressed, neutral, restless
- VOICE_OPTIONS has 3 entries with valid ElevenLabs voice IDs
- ClientMessage start_session type includes mood and voiceId
- OrchestratorOptions includes mood field
- buildPhaseInstructions accepts 3 params (phase, transitionHint?, moodContext?)
- session-handler passes voiceId to synthesizeSentence
</verification>

<success_criteria>
Server-side mood adaptation and voice selection are wired end-to-end. A WebSocket client can send `{ type: "start_session", mood: "anxious", voiceId: "21m00Tcm4TlvDq8ikWAM" }` and the orchestrator will inject anxious mood context into every phase prompt, while TTS will use Rachel's voice for every sentence.
</success_criteria>

<output>
After completion, create `.planning/phases/09-differentiators-polish/09-01-SUMMARY.md`
</output>
