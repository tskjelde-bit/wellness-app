---
phase: 09-differentiators-polish
plan: 03
type: execute
wave: 2
depends_on: ["09-01", "09-02"]
files_modified:
  - src/components/session/mood-selector.tsx
  - src/components/session/voice-picker.tsx
  - src/components/session/post-session-screen.tsx
  - src/components/session/pre-session-flow.tsx
  - src/components/session/session-screen.tsx
  - src/hooks/use-session-ws.ts
autonomous: true
requirements: [DIFF-01, DIFF-02, DIFF-03, DIFF-04, DIFF-05]

must_haves:
  truths:
    - "User can select a mood before the session and the AI adapts its guidance accordingly"
    - "User can choose from 3 curated voice options before the session"
    - "User can select a background soundscape that loops during the session"
    - "User can independently adjust voice and ambient volume during the session"
    - "User sees reflection prompts and grounding exercises after a session ends"
  artifacts:
    - path: "src/components/session/mood-selector.tsx"
      provides: "Mood selection grid component"
      contains: "MoodSelector"
    - path: "src/components/session/voice-picker.tsx"
      provides: "Voice selection component"
      contains: "VoicePicker"
    - path: "src/components/session/post-session-screen.tsx"
      provides: "Post-session aftercare screen"
      contains: "PostSessionScreen"
    - path: "src/components/session/pre-session-flow.tsx"
      provides: "Extended pre-session flow with mood and voice steps"
      contains: "mood"
    - path: "src/components/session/session-screen.tsx"
      provides: "Session screen with ambient audio, mixer, and post-session transition"
      contains: "ambientGain"
  key_links:
    - from: "src/components/session/pre-session-flow.tsx"
      to: "src/components/session/session-screen.tsx"
      via: "onBegin callback with mood, voiceId, soundscape"
      pattern: "onBegin.*mood.*voiceId"
    - from: "src/components/session/session-screen.tsx"
      to: "src/hooks/use-session-ws.ts"
      via: "startSession with mood and voiceId options"
      pattern: "startSession.*mood.*voiceId"
    - from: "src/components/session/session-screen.tsx"
      to: "src/hooks/use-ambient-audio.ts"
      via: "startSoundscape called after connection with selected soundscape"
      pattern: "startSoundscape"
---

<objective>
Wire all Phase 9 differentiators into the client UI: mood selection, voice selection, soundscape selection, volume mixer, and post-session aftercare.

Purpose: Complete the end-to-end user experience for all five differentiators (DIFF-01 through DIFF-05). Users select mood, voice, and soundscape before a session; control volumes during the session; and receive aftercare content after the session.

Output: MoodSelector, VoicePicker, PostSessionScreen components. Extended PreSessionFlow with 4 steps. Extended SessionScreen with ambient audio, mixer, and post-session state.
</objective>

<execution_context>
@/Users/torbjorntest/.claude/get-shit-done/workflows/execute-plan.md
@/Users/torbjorntest/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-differentiators-polish/09-RESEARCH.md
@.planning/phases/09-differentiators-polish/09-01-SUMMARY.md
@.planning/phases/09-differentiators-polish/09-02-SUMMARY.md
@.planning/phases/07-session-ux-controls/07-01-SUMMARY.md
@.planning/phases/07-session-ux-controls/07-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MoodSelector, VoicePicker, and PostSessionScreen components</name>
  <files>
    src/components/session/mood-selector.tsx
    src/components/session/voice-picker.tsx
    src/components/session/post-session-screen.tsx
  </files>
  <action>
1. Create `src/components/session/mood-selector.tsx`:
   - `"use client"` directive.
   - Import `MOOD_OPTIONS` from `@/lib/session/mood-prompts`.
   - Props: `{ selected: string; onSelect: (mood: string) => void }`.
   - Render a grid of mood buttons (5 items, use `grid-cols-2` with the 5th item spanning full width, or use `flex flex-wrap justify-center gap-2`).
   - Each button shows the emoji + label from MOOD_OPTIONS.
   - Selected state: `bg-rose text-white`, unselected: `bg-cream/10 text-cream/70 hover:bg-cream/15` (matching the length selection pattern from PreSessionFlow).
   - Min button height: `min-h-[48px]` for touch targets.

2. Create `src/components/session/voice-picker.tsx`:
   - `"use client"` directive.
   - Import `VOICE_OPTIONS` from `@/lib/tts/elevenlabs-client`.
   - Props: `{ selected: string; onSelect: (voiceId: string) => void }`.
   - Render 3 voice option cards in a vertical stack (`flex flex-col gap-3`).
   - Each card shows: name (bold), preview description (text-sm text-cream/50).
   - Selected state: `border-rose bg-rose/10`, unselected: `border-cream/10 bg-cream/5`.
   - Use `rounded-xl border p-4` for card styling.

3. Create `src/components/session/post-session-screen.tsx`:
   - `"use client"` directive.
   - Export `PostSessionScreen` with props: `{ onDone: () => void }`.
   - Define static `GROUNDING_EXERCISES` array (3 exercises from research: 5-4-3-2-1 Senses, Body Check-In, Three Breaths -- each with title and description).
   - Define static `REFLECTION_PROMPTS` array (4 prompts from research).
   - On mount, randomly select 1 grounding exercise and 1 reflection prompt using `Math.random()`.
   - Layout: dark charcoal bg (matching session screen), centered content, `animate-fade-in`.
   - Show a warm heading: "Take a moment..." or "Your session is complete".
   - Display the selected grounding exercise (title + description) in a card.
   - Display the selected reflection prompt in italic text.
   - "Return to Dashboard" button at the bottom using `bg-rose` primary style.
   - The `onDone` callback navigates away from the session.
  </action>
  <verify>
    Run `npx tsc --noEmit` to verify no type errors. Verify all three components export correctly.
  </verify>
  <done>
    MoodSelector renders 5 mood options with selection state. VoicePicker renders 3 voice cards with selection state. PostSessionScreen shows random grounding exercise and reflection prompt with a return-to-dashboard button.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend PreSessionFlow and SessionScreen for full differentiator integration</name>
  <files>
    src/components/session/pre-session-flow.tsx
    src/components/session/session-screen.tsx
    src/hooks/use-session-ws.ts
  </files>
  <action>
1. Extend `useSessionWebSocket` in `src/hooks/use-session-ws.ts`:
   - Add `sessionEnded` boolean state (default false). Set to `true` when `session_end` message is received (in the existing switch case), BEFORE calling `stopAudio()`.
   - Return `sessionEnded` from the hook.
   - Extend `startSession` to accept `mood?: string` and `voiceId?: string` in its options object (alongside existing `prompt` and `sessionLength`). Send them in the JSON message.

2. Extend `PreSessionFlow` in `src/components/session/pre-session-flow.tsx`:
   - Import `MoodSelector` from `./mood-selector` and `VoicePicker` from `./voice-picker`.
   - Import `SOUNDSCAPE_OPTIONS` from `@/hooks/use-ambient-audio`.
   - Import `DEFAULT_VOICE_ID` from `@/lib/tts/elevenlabs-client`.
   - Extend the `Step` type to: `"mood" | "voice" | "length" | "consent"` (mood and voice BEFORE length selection).
   - Add state: `selectedMood` (default "neutral"), `selectedVoiceId` (default `DEFAULT_VOICE_ID`), `selectedSoundscape` (default "silence").
   - Update the `onBegin` callback interface to include `mood: string`, `voiceId: string`, `soundscape: string` alongside existing `sessionLength` and `sensoryConsent`.
   - Step "mood": Heading "How are you feeling?", subtext "This helps me tailor your session". Render `MoodSelector` with selected/onSelect. "Continue" button advances to "voice" step.
   - Step "voice": Heading "Choose your guide's voice". Render `VoicePicker` with selected/onSelect. "Continue" button advances to "length" step.
   - Step "length": Same as current, but add a soundscape selector below the duration grid. Render soundscape options as a horizontal scrollable row of small pills (`flex gap-2 overflow-x-auto`). Each pill shows the soundscape label. Selected pill uses `bg-blush/20 text-cream/80 border-blush`, unselected uses `bg-cream/5 text-cream/50 border-cream/10`. "Continue" advances to "consent".
   - Step "consent": Same as current.
   - Update the consent `onBegin` call and `handleSkipSensory` to pass all selections: `{ sessionLength, sensoryConsent, mood: selectedMood, voiceId: selectedVoiceId, soundscape: selectedSoundscape }`.
   - Lift ALL selection state to PreSessionFlow parent to prevent loss between steps (Research Pitfall 6).

3. Extend `SessionScreen` in `src/components/session/session-screen.tsx`:
   - Import `useAmbientAudio` from `@/hooks/use-ambient-audio`.
   - Import `VolumeMixer` from `./volume-mixer`.
   - Import `PostSessionScreen` from `./post-session-screen`.
   - Import `useRouter` from `next/navigation`.
   - Destructure new fields from useSessionWebSocket: `sessionEnded`, and the new `audioContext`, `voiceGain`, `ambientGain` from useAudioQueue (these are exposed from useSessionWebSocket internally, OR you may need to restructure slightly).

   APPROACH for audio nodes: Since useSessionWebSocket wraps useAudioQueue internally, the simplest approach is:
   - In SessionScreen, call `useAudioQueue()` directly and pass its outputs to `useSessionWebSocket` -- BUT this breaks the current encapsulation. INSTEAD: extend `useSessionWebSocket` to expose `audioContext`, `voiceGain`, `ambientGain` from the internal `useAudioQueue` call. Add these three to the returned object.

   - Add state: `selectedMood`, `selectedVoiceId`, `selectedSoundscape` (set from handleBegin).
   - Add `showMixer` boolean state (default false) toggled by a small mixer icon button near the session controls.
   - Call `useAmbientAudio(audioContext, ambientGain)` to get `{ startSoundscape, stopSoundscape }`.
   - In `handleBegin`, store mood, voiceId, soundscape selections.
   - In the `useEffect` that calls `startSession`, pass `mood` and `voiceId` in the options.
   - Add a second `useEffect`: when `isConnected` becomes true AND selectedSoundscape is not "silence", call `startSoundscape(selectedSoundscape)`. This starts the ambient loop after AudioContext is active (Research Pitfall 1: must be after user gesture).
   - In the active session view:
     - Add a small mixer toggle button (a volume icon, tiny like the end session button) near the bottom controls.
     - Conditionally render `VolumeMixer` when `showMixer` is true, positioned above the controls.
   - When `sessionEnded` is true, render `PostSessionScreen` with `onDone={() => router.push("/dashboard")}`. Stop the ambient soundscape in a useEffect when sessionEnded becomes true.
   - Clean up: stop soundscape on unmount.
  </action>
  <verify>
    Run `npx tsc --noEmit` to verify no type errors. Verify PreSessionFlow has 4 steps. Verify SessionScreen renders VolumeMixer and PostSessionScreen. Verify startSession sends mood and voiceId.
  </verify>
  <done>
    PreSessionFlow guides user through mood -> voice -> length + soundscape -> consent. SessionScreen starts ambient audio after connection, shows volume mixer toggle, and transitions to PostSessionScreen after session ends. All 5 differentiators (DIFF-01 through DIFF-05) are wired end-to-end in the client.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with no errors
- PreSessionFlow renders 4 steps: mood, voice, length (with soundscape), consent
- SessionScreen starts ambient soundscape after WebSocket connects
- VolumeMixer appears when toggled during active session
- PostSessionScreen shows after session_end with grounding exercise and reflection prompt
- startSession WebSocket message includes mood and voiceId
- All state (mood, voice, soundscape, length) persists across PreSessionFlow steps
</verification>

<success_criteria>
Complete end-to-end user flow: User selects mood, voice, session length, and soundscape in PreSessionFlow. During the session, ambient audio loops in background with independent volume control via mixer. AI adapts tone to selected mood. After session ends, user sees aftercare content with grounding and reflection. All 5 DIFF requirements are met.
</success_criteria>

<output>
After completion, create `.planning/phases/09-differentiators-polish/09-03-SUMMARY.md`
</output>
